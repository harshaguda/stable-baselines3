{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CustomACPolicy' from 'stable_baselines3.ppo.policies' (/home/irobotics/stable-baselines3/stable_baselines3/ppo/policies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAC, A2C, PPO\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_atari_env\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecFrameStack\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/__init__.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQN\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mher\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mher_replay_buffer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HerReplayBuffer\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msac\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAC\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtd3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TD3\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/ppo/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CnnPolicy, MlpPolicy, MultiInputPolicy, CustomACPolicy\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from stable_baselines3.ppo.custom_policies import CustomActorCriticPolicy, CustomFeaturesExtractor, CustomNetwork\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# , \"CustomActorCriticPolicy\", \"CustomFeaturesExtractor\", \"CustomNetwork\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CustomACPolicy' from 'stable_baselines3.ppo.policies' (/home/irobotics/stable-baselines3/stable_baselines3/ppo/policies.py)"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "from stable_baselines3.common.type_aliases import GymStepReturn\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pytest\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces.space import Space\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from stable_baselines3 import SAC, A2C, PPO\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "# from stable_baselines3.sac.custom_policies import Cus, CustomContinousCritic, CustomSACPolicy\n",
    "from stable_baselines3.ppo.custom_policies import CustomActorCriticPolicy\n",
    "# from stable_baselines3 import \n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeImageEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Fake image environment for testing purposes, it mimics Atari games.\n",
    "\n",
    "    :param action_dim: Number of discrete actions\n",
    "    :param screen_height: Height of the image\n",
    "    :param screen_width: Width of the image\n",
    "    :param n_channels: Number of color channels\n",
    "    :param discrete: Create discrete action space instead of continuous\n",
    "    :param channel_first: Put channels on first axis instead of last\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_dim: int = 6,\n",
    "        screen_height: int = 64,\n",
    "        screen_width: int = 64,\n",
    "        n_channels: int = 3,\n",
    "        discrete: bool = False,\n",
    "        channel_first: bool = False,\n",
    "    ) -> None:\n",
    "        self.observation_shape = (screen_height, screen_width, n_channels)\n",
    "        if channel_first:\n",
    "            self.observation_shape = (n_channels, screen_height, screen_width)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=self.observation_shape, dtype=np.uint8)\n",
    "        if discrete:\n",
    "            self.action_space = spaces.Discrete(action_dim)\n",
    "        else:\n",
    "            self.action_space = spaces.Box(low=-1, high=1, shape=(action_dim,), dtype=np.float32)\n",
    "        self.ep_length = 10\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        return self.observation_space.sample(), {}\n",
    "\n",
    "    def step(self, action: Union[np.ndarray, int]) -> GymStepReturn:\n",
    "        reward = 0.0\n",
    "        self.current_step += 1\n",
    "        terminated = False\n",
    "        truncated = self.current_step >= self.ep_length\n",
    "        return self.observation_space.sample(), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode: str = \"human\") -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1,3,64,64).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irobotics/stable-baselines3/sb3-dev/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irobotics/stable-baselines3/sb3-dev/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "env = FakeImageEnv(discrete=False)\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class somePolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1), #64*64*64\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1), #32*32*32\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1), #16*32*32\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(8 * 16 * 16, 6) # 8*16*16\n",
    "                )\n",
    "    def forward(self, x):\n",
    "            x /= 255.\n",
    "            return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = somePolicy().to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10       |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 440      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/common/on_policy_algorithm.py:299\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/ppo/ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    208\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/common/buffers.py:504\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    502\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/common/buffers.py:520\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[0;34m(self, batch_inds, env)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    509\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    510\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[1;32m    512\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m    519\u001b[0m     )\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/stable-baselines3/stable_baselines3/common/buffers.py:138\u001b[0m, in \u001b[0;36mBaseBuffer.to_torch\u001b[0;34m(self, array, copy)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mConvert a numpy array to a PyTorch tensor.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mNote: it copies the data by default\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(array, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
